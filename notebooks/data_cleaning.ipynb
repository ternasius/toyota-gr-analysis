{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14674728",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a1f56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2556d83",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d216ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "RAW_BASE = \"../datasets/circuit-of-the-americas/COTA/Race 1\"\n",
    "OUTPUT_ROOT = os.path.abspath(\"../analysis_by_driver_vec\")\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "ANALYSIS_FILE = os.path.join(RAW_BASE, \"23_AnalysisEnduranceWithSections_Race 1_Anonymized.csv\")\n",
    "WEATHER_FILE = os.path.join(RAW_BASE, \"26_Weather_Race 1_Anonymized.csv\")\n",
    "TELEMETRY_FILE = os.path.join(RAW_BASE, \"R1_cota_telemetry_data.csv\")\n",
    "\n",
    "LAP_START_FILE = os.path.join(RAW_BASE, \"COTA_lap_start_time_R1.csv\")\n",
    "LAP_END_FILE = os.path.join(RAW_BASE, \"COTA_lap_end_time_R1.csv\")\n",
    "LAP_TIME_FILE = os.path.join(RAW_BASE, \"COTA_lap_time_R1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b35bbd",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbb5f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def time_to_seconds(t):\n",
    "    \"\"\"Convert strings like 'M:SS.mmm' or 'H:MM:SS.mmm' or 'MM:SS' to seconds (float).\n",
    "    Returns NaN for missing or unparsable inputs.\"\"\"\n",
    "    if pd.isna(t):\n",
    "        return np.nan\n",
    "    s = str(t).strip()\n",
    "    if s == '':\n",
    "        return np.nan\n",
    "    # replace comma decimal separators\n",
    "    s = s.replace(',', '.')\n",
    "    parts = s.split(':')\n",
    "    try:\n",
    "        if len(parts) == 1:\n",
    "            return float(parts[0])\n",
    "        if len(parts) == 2:\n",
    "            m, sec = parts\n",
    "            return float(m) * 60.0 + float(sec)\n",
    "        if len(parts) == 3:\n",
    "            h, m, sec = parts\n",
    "            return float(h) * 3600.0 + float(m) * 60.0 + float(sec)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce3327",
   "metadata": {},
   "source": [
    "## Cleaning Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b11b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- 1) Load & clean \"analysis with sections\" ----------------------\n",
    "print(\"Loading analysis file:\", ANALYSIS_FILE)\n",
    "analysis_df = pd.read_csv(ANALYSIS_FILE, sep=';', dtype=str)\n",
    "analysis_df = clean_column_names(analysis_df)\n",
    "print(\"Raw columns:\", analysis_df.columns.tolist())\n",
    "\n",
    "# Drop fully-empty columns\n",
    "analysis_df = analysis_df.dropna(how='all', axis=1)\n",
    "\n",
    "# Drop unwanted columns to make files readable in Excel\n",
    "for drop_col in ['CLASS', 'GROUP', 'MANUFACTURER']:\n",
    "    if drop_col in analysis_df.columns:\n",
    "        analysis_df = analysis_df.drop(columns=[drop_col])\n",
    "\n",
    "# Convert numeric-ish columns\n",
    "for col in ['NUMBER', 'DRIVER_NUMBER', 'LAP_NUMBER', 'KPH', 'TOP_SPEED', 'PIT_TIME']:\n",
    "    if col in analysis_df.columns:\n",
    "        analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')\n",
    "\n",
    "# Convert LAP times and sector strings to seconds\n",
    "for col in ['LAP_TIME', 'S1', 'S2', 'S3', 'S1_SECONDS', 'S2_SECONDS', 'S3_SECONDS']:\n",
    "    if col in analysis_df.columns:\n",
    "        # prefer S?_SECONDS if present numeric; otherwise convert textual S1 etc.\n",
    "        if col.endswith('_SECONDS'):\n",
    "            analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')\n",
    "        else:\n",
    "            analysis_df[col + '_SEC'] = analysis_df[col].apply(time_to_seconds)\n",
    "\n",
    "# If explicit seconds columns exist but not the _SEC ones, copy them\n",
    "for s_col in ['S1_SECONDS', 'S2_SECONDS', 'S3_SECONDS']:\n",
    "    if s_col in analysis_df.columns and (s_col.replace('_SECONDS', '') + '_SEC') not in analysis_df.columns:\n",
    "        analysis_df[s_col.replace('_SECONDS', '') + '_SEC'] = pd.to_numeric(analysis_df[s_col], errors='coerce')\n",
    "\n",
    "# Normalize flag column\n",
    "if 'FLAG_AT_FL' in analysis_df.columns:\n",
    "    analysis_df['FLAG_AT_FL'] = analysis_df['FLAG_AT_FL'].astype(str).str.strip()\n",
    "\n",
    "# Create a numeric Lap_Time if not present\n",
    "if 'LAP_TIME_SEC' not in analysis_df.columns and 'LAP_TIME' in analysis_df.columns:\n",
    "    analysis_df['LAP_TIME_SEC'] = analysis_df['LAP_TIME'].apply(time_to_seconds)\n",
    "\n",
    "print(\"Loaded analysis rows:\", len(analysis_df))\n",
    "\n",
    "# ---------------------- 2) Filter out unwanted laps ----------------------\n",
    "# Remove laps under FCY/SC/AUTO flags and laps that finished in pit\n",
    "bad_flags = set(['FCY', 'SC', 'YELLOW'])\n",
    "if 'FLAG_AT_FL' in analysis_df.columns:\n",
    "    before = len(analysis_df)\n",
    "    analysis_df = analysis_df[~analysis_df['FLAG_AT_FL'].isin(bad_flags)]\n",
    "    print(f\"Filtered by flag: {before} -> {len(analysis_df)}\")\n",
    "\n",
    "# Proper pit filtering: 'B' marks a pit entry lap, '0' means normal lap\n",
    "if 'CROSSING_FINISH_LINE_IN_PIT' in analysis_df.columns:\n",
    "    before = len(analysis_df)\n",
    "    analysis_df['CROSSING_FINISH_LINE_IN_PIT'] = analysis_df['CROSSING_FINISH_LINE_IN_PIT'].astype(str).str.strip()\n",
    "    analysis_df = analysis_df[analysis_df['CROSSING_FINISH_LINE_IN_PIT'] != 'B']\n",
    "    print(f\"Filtered pit-crossing laps: {before} -> {len(analysis_df)}\")\n",
    "\n",
    "# Remove clearly invalid lap times\n",
    "if 'LAP_TIME_SEC' in analysis_df.columns:\n",
    "    before = len(analysis_df)\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df['LAP_TIME_SEC'], errors='coerce') > 5.0] # remove zeros and extremely small\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df['LAP_TIME_SEC'], errors='coerce') < 3600.0] # sanity upper bound\n",
    "    print(f\"Filtered by lap time sanity: {before} -> {len(analysis_df)}\")\n",
    "\n",
    "# ---------------------- 3) Save cleaned per-driver CSVs (compact columns) ----------------------\n",
    "# Select a useful subset of columns for driver-coaching CSVs\n",
    "default_cols = [\n",
    "'NUMBER','DRIVER_NUMBER','LAP_NUMBER','LAP_TIME','LAP_TIME_SEC',\n",
    "'S1','S1_SEC','S2','S2_SEC','S3','S3_SEC',\n",
    "'KPH','TOP_SPEED','ELAPSED','HOUR','FLAG_AT_FL'\n",
    "]\n",
    "cols_to_save = [c for c in default_cols if c in analysis_df.columns]\n",
    "print('Columns saved per driver:', cols_to_save)\n",
    "\n",
    "# Ensure NUMBER is numeric for filename safety\n",
    "if 'NUMBER' in analysis_df.columns:\n",
    "    analysis_df['NUMBER'] = pd.to_numeric(analysis_df['NUMBER'], errors='coerce')\n",
    "\n",
    "for num, grp in analysis_df.groupby('NUMBER'):\n",
    "    safe_num = int(num) if not pd.isna(num) else 'unknown'\n",
    "    out_path = os.path.join(OUTPUT_ROOT, f\"driver_{safe_num}.csv\")\n",
    "    grp[cols_to_save].to_csv(out_path, index=False, sep=',')\n",
    "    print(f\"Saved driver file: {out_path} ({len(grp)} laps)\")\n",
    "\n",
    "# ---------------------- 4) Load telemetry (if present) and pivot into wide form ----------------------\n",
    "if os.path.exists(TELEMETRY_FILE):\n",
    "    print('Loading telemetry (this may be large):', TELEMETRY_FILE)\n",
    "    tel_df = pd.read_csv(TELEMETRY_FILE)\n",
    "    tel_df = clean_column_names(tel_df)\n",
    "    # Parse times\n",
    "    for tcol in ['meta_time','timestamp']:\n",
    "        if tcol in tel_df.columns:\n",
    "            tel_df[tcol] = pd.to_datetime(tel_df[tcol], errors='coerce', utc=True)\n",
    "    # Ensure values numeric when possible\n",
    "    tel_df['telemetry_value'] = pd.to_numeric(tel_df['telemetry_value'], errors='coerce')\n",
    "\n",
    "    # Pivot: make each telemetry_name a column per vehicle_id + timestamp\n",
    "    index_cols = [c for c in ['vehicle_id','meta_time','timestamp','lap'] if c in tel_df.columns]\n",
    "    print('Telemetry index cols:', index_cols)\n",
    "    tel_wide = tel_df.pivot_table(index=index_cols, columns='telemetry_name', values='telemetry_value', aggfunc='first').reset_index()\n",
    "    print('Telemetry wide shape:', tel_wide.shape)\n",
    "\n",
    "    # Save per-vehicle telemetry sampled per-lap timestamp rows (if lap info exists)\n",
    "    if 'vehicle_id' in tel_wide.columns:\n",
    "        telem_out = os.path.join(OUTPUT_ROOT, 'telemetry_per_timestamp.csv')\n",
    "        tel_wide.to_csv(telem_out, index=False)\n",
    "        print('Saved telemetry per-timestamp wide file to:', telem_out)\n",
    "else:\n",
    "    print('No telemetry file found at', TELEMETRY_FILE)\n",
    "\n",
    "# ---------------------- 5) Compute per-lap telemetry summaries (vectorized) ----------------------\n",
    "print(\"Generating per-lap and per-sector telemetry summary for MAD-filtered laps...\")\n",
    "\n",
    "if os.path.exists(TELEMETRY_FILE):\n",
    "    tel_df = pd.read_csv(TELEMETRY_FILE)\n",
    "    tel_df = clean_column_names(tel_df)\n",
    "    tel_df['timestamp'] = pd.to_datetime(tel_df['timestamp'], errors='coerce', utc=True)\n",
    "    tel_df['vehicle_id'] = tel_df['vehicle_id'].astype(str)\n",
    "    tel_df = tel_df.dropna(subset=['timestamp', 'telemetry_value'])\n",
    "    tel_df['telemetry_value'] = pd.to_numeric(tel_df['telemetry_value'], errors='coerce')\n",
    "\n",
    "    # Keep only valid laps from driver_session_stats\n",
    "    driver_stats_path = os.path.join(OUTPUT_ROOT, \"driver_session_stats.csv\")\n",
    "    if os.path.exists(driver_stats_path):\n",
    "        valid_laps_df = pd.read_csv(driver_stats_path)\n",
    "        valid_pairs = analysis_df.merge(\n",
    "            valid_laps_df[['DriverNumber']],\n",
    "            left_on='NUMBER', right_on='DriverNumber',\n",
    "            how='inner'\n",
    "        )\n",
    "        valid_laps = set(zip(valid_pairs['NUMBER'].astype(str), valid_pairs['LAP_NUMBER']))\n",
    "        tel_df = tel_df[tel_df.apply(lambda r: (r['vehicle_id'], r.get('lap', np.nan)) in valid_laps, axis=1)]\n",
    "\n",
    "    # Compute per-lap, per-sector metrics\n",
    "    per_lap_list = []\n",
    "    for (vid, lap), group in tel_df.groupby(['vehicle_id','lap']):\n",
    "        lap_metrics = {'vehicle_id': vid, 'lap': lap}\n",
    "\n",
    "        # Whole-lap metrics\n",
    "        lap_metrics['mean_throttle'] = group.loc[group['telemetry_name'].isin(['ath','aps','throttle']),'telemetry_value'].mean()\n",
    "        lap_metrics['mean_brake'] = group.loc[group['telemetry_name'].isin(['pbrake_f','pbrake_r','brake']),'telemetry_value'].mean()\n",
    "        sa = group.loc[group['telemetry_name']=='Steering_Angle','telemetry_value']\n",
    "        lap_metrics['steering_smoothness'] = sa.diff().abs().mean() if not sa.empty else np.nan\n",
    "\n",
    "        # Optional: sector metrics if sector telemetry columns exist\n",
    "        for sector in ['S1','S2','S3']:\n",
    "            sector_mask = group['telemetry_name'].str.contains(sector, case=False)\n",
    "            if sector_mask.any():\n",
    "                lap_metrics[f'{sector}_mean_throttle'] = group.loc[group['telemetry_name'].isin(['ath','aps','throttle']) & sector_mask,'telemetry_value'].mean()\n",
    "                lap_metrics[f'{sector}_mean_brake'] = group.loc[group['telemetry_name'].isin(['pbrake_f','pbrake_r','brake']) & sector_mask,'telemetry_value'].mean()\n",
    "                sa_sector = group.loc[sector_mask & (group['telemetry_name']=='Steering_Angle'),'telemetry_value']\n",
    "                lap_metrics[f'{sector}_steering_smoothness'] = sa_sector.diff().abs().mean() if not sa_sector.empty else np.nan\n",
    "\n",
    "        per_lap_list.append(lap_metrics)\n",
    "\n",
    "    if per_lap_list:\n",
    "        per_lap_df = pd.DataFrame(per_lap_list)\n",
    "        per_lap_path = os.path.join(OUTPUT_ROOT, 'per_lap_telemetry_summary.csv')\n",
    "        per_lap_df.to_csv(per_lap_path, index=False)\n",
    "        print('✅ Saved per-lap telemetry summary to:', per_lap_path)\n",
    "else:\n",
    "    print('⚠️ No telemetry data matched any valid MAD-filtered laps.')\n",
    "\n",
    "# ---------------------- 6) Quick driver session statistics (filter extreme laps) ----------------------\n",
    "print(\"\\nComputing driver session statistics with sanity filtering...\")\n",
    "\n",
    "# Ensure numeric\n",
    "for col in ['LAP_TIME_SEC', 'S1_SEC', 'S2_SEC', 'S3_SEC']:\n",
    "    if col in analysis_df.columns:\n",
    "        analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')\n",
    "\n",
    "# Filter obviously invalid or extreme laps (more than 15 sec slower than driver mean)\n",
    "if 'NUMBER' in analysis_df.columns:\n",
    "    filtered_df_list = []\n",
    "    for driver, grp in analysis_df.groupby('NUMBER'):\n",
    "        grp = grp.copy()\n",
    "        median = grp['LAP_TIME_SEC'].median()\n",
    "        mad = (grp['LAP_TIME_SEC'] - median).abs().median()\n",
    "        cutoff = median + 3 * mad  # 3 MADs above median\n",
    "\n",
    "        # Debug info\n",
    "        print(f\"\\nDriver {driver}:\")\n",
    "        print(f\"  Number of laps before filtering: {len(grp)}\")\n",
    "        print(f\"  Median lap time: {median:.3f} sec\")\n",
    "        print(f\"  MAD (Median Abs Deviation): {mad:.3f} sec\")\n",
    "        print(f\"  Cutoff for valid laps: {cutoff:.3f} sec\")\n",
    "\n",
    "        # Apply filtering\n",
    "        filtered_grp = grp[grp['LAP_TIME_SEC'] <= cutoff]\n",
    "        print(f\"  Number of laps after filtering: {len(filtered_grp)}\")\n",
    "        if len(filtered_grp) < len(grp):\n",
    "            removed = len(grp) - len(filtered_grp)\n",
    "            print(f\"  → Removed {removed} lap(s) as outliers.\")\n",
    "\n",
    "        filtered_df_list.append(filtered_grp)\n",
    "\n",
    "    valid_laps_df = pd.concat(filtered_df_list, ignore_index=True)\n",
    "else:\n",
    "    valid_laps_df = analysis_df.copy()\n",
    "\n",
    "# Compute per-driver stats\n",
    "agg_dict = {\n",
    "    'LAP_TIME_SEC': ['count','min','mean','std'],\n",
    "    'S1_SEC': 'min', 'S2_SEC': 'min', 'S3_SEC': 'min'\n",
    "}\n",
    "driver_stats_df = valid_laps_df.groupby('NUMBER', as_index=False).agg(agg_dict)\n",
    "\n",
    "# Flatten columns\n",
    "driver_stats_df.columns = [\n",
    "    'DriverNumber','Laps','BestLap(s)','AvgLap(s)','StdDev(s)',\n",
    "    'S1Best','S2Best','S3Best'\n",
    "]\n",
    "driver_stats_df['TheoreticalBest(s)'] = driver_stats_df[['S1Best','S2Best','S3Best']].sum(axis=1)\n",
    "\n",
    "# Round\n",
    "driver_stats_df[['BestLap(s)','AvgLap(s)','StdDev(s)','TheoreticalBest(s)']] = \\\n",
    "    driver_stats_df[['BestLap(s)','AvgLap(s)','StdDev(s)','TheoreticalBest(s)']].round(3)\n",
    "\n",
    "# Save CSV\n",
    "driver_stats_path = os.path.join(OUTPUT_ROOT, \"driver_session_stats.csv\")\n",
    "driver_stats_df.to_csv(driver_stats_path, index=False)\n",
    "print(f\"✅ Saved driver session stats to: {driver_stats_path}\")\n",
    "display(driver_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d88cc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Filter extreme laps using MAD per driver ----------------------\n",
    "if 'NUMBER' in analysis_df.columns:\n",
    "    filtered_df_list = []\n",
    "    driver_filter_summary = []  # keep record for debugging/inspection\n",
    "\n",
    "    for driver, grp in analysis_df.groupby('NUMBER'):\n",
    "        grp = grp.copy()\n",
    "        median = grp['LAP_TIME_SEC'].median()\n",
    "        mad = (grp['LAP_TIME_SEC'] - median).abs().median()\n",
    "        cutoff = median + 3 * mad  # 3 MADs above median\n",
    "\n",
    "        # Filter outliers\n",
    "        filtered_grp = grp[grp['LAP_TIME_SEC'] <= cutoff]\n",
    "\n",
    "        # Collect summary info\n",
    "        driver_filter_summary.append({\n",
    "            'DriverNumber': driver,\n",
    "            'TotalLaps': len(grp),\n",
    "            'ValidLaps': len(filtered_grp),\n",
    "            'RemovedLaps': len(grp) - len(filtered_grp),\n",
    "            'MedianLap(s)': median,\n",
    "            'MAD(s)': mad,\n",
    "            'Cutoff(s)': cutoff\n",
    "        })\n",
    "\n",
    "        print(f\"\\nDriver {driver}:\")\n",
    "        print(f\"  Total laps: {len(grp)} | Valid: {len(filtered_grp)} | Removed: {len(grp) - len(filtered_grp)}\")\n",
    "        print(f\"  Median: {median:.3f}s | MAD: {mad:.3f}s | Cutoff: {cutoff:.3f}s\")\n",
    "\n",
    "        # Save filtered laps to individual CSV\n",
    "        safe_num = int(driver) if not pd.isna(driver) else 'unknown'\n",
    "        out_path = os.path.join(OUTPUT_ROOT, f\"driver_{safe_num}.csv\")\n",
    "        filtered_grp.to_csv(out_path, index=False)\n",
    "        print(f\"  ✅ Saved filtered driver file: {out_path} ({len(filtered_grp)} valid laps)\")\n",
    "\n",
    "        filtered_df_list.append(filtered_grp)\n",
    "\n",
    "    # Combine all valid laps for session-level stats\n",
    "    valid_laps_df = pd.concat(filtered_df_list, ignore_index=True)\n",
    "\n",
    "    # Optional summary overview\n",
    "    filter_summary_df = pd.DataFrame(driver_filter_summary)\n",
    "    print(\"\\nSummary of filtering across drivers:\")\n",
    "    display(filter_summary_df)\n",
    "else:\n",
    "    valid_laps_df = analysis_df.copy()\n",
    "\n",
    "# ---------------------- Compute per-driver statistics (only valid laps) ----------------------\n",
    "agg_dict = {\n",
    "    'LAP_TIME_SEC': ['count','min','mean','std'],\n",
    "    'S1_SEC': 'min', 'S2_SEC': 'min', 'S3_SEC': 'min'\n",
    "}\n",
    "\n",
    "driver_stats_df = valid_laps_df.groupby('NUMBER', as_index=False).agg(agg_dict)\n",
    "driver_stats_df.columns = [\n",
    "    'DriverNumber','Laps','BestLap(s)','AvgLap(s)','StdDev(s)',\n",
    "    'S1Best','S2Best','S3Best'\n",
    "]\n",
    "driver_stats_df['TheoreticalBest(s)'] = driver_stats_df[['S1Best','S2Best','S3Best']].sum(axis=1)\n",
    "\n",
    "# Round for readability\n",
    "driver_stats_df[['BestLap(s)','AvgLap(s)','StdDev(s)','TheoreticalBest(s)']] = \\\n",
    "    driver_stats_df[['BestLap(s)','AvgLap(s)','StdDev(s)','TheoreticalBest(s)']].round(3)\n",
    "\n",
    "# Save updated session-level stats\n",
    "driver_stats_path = os.path.join(OUTPUT_ROOT, \"driver_session_stats.csv\")\n",
    "driver_stats_df.to_csv(driver_stats_path, index=False)\n",
    "print(f\"\\n✅ Saved filtered driver session stats to: {driver_stats_path}\")\n",
    "display(driver_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb2339",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "telemetry_file = \"../analysis_by_driver_vec/per_lap_telemetry_summary.csv\"\n",
    "\n",
    "# Load the telemetry summary\n",
    "telemetry_df = pd.read_csv(telemetry_file)\n",
    "\n",
    "# Extract driver number from vehicle_id (last number after dash)\n",
    "telemetry_df['NUMBER'] = telemetry_df['vehicle_id'].str.split('-').str[-1].astype(int)\n",
    "\n",
    "filtered_list = []\n",
    "\n",
    "for driver, grp in telemetry_df.groupby('NUMBER'):\n",
    "    median = grp['mean_throttle'].median()\n",
    "    mad = (grp['mean_throttle'] - median).abs().median()\n",
    "    # Define low-throttle cutoff (e.g., median - 3*MAD)\n",
    "    cutoff = median - 3 * mad\n",
    "    grp_filtered = grp[grp['mean_throttle'] >= cutoff]\n",
    "    filtered_list.append(grp_filtered)\n",
    "    print(f\"Driver {driver}: median={median:.2f}, MAD={mad:.2f}, cutoff={cutoff:.2f}, removed={len(grp)-len(grp_filtered)} laps\")\n",
    "\n",
    "# Concatenate filtered groups\n",
    "filtered_telemetry_df = pd.concat(filtered_list, ignore_index=True)\n",
    "\n",
    "# Overwrite the original CSV\n",
    "filtered_telemetry_df.to_csv(telemetry_file, index=False)\n",
    "\n",
    "print(f\"Filtered telemetry saved. Remaining rows: {len(filtered_telemetry_df)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
